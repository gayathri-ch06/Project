{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DNN.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1ROE72Xp7JQdtbfs0bqZ-gLwemt2EGbVB","authorship_tag":"ABX9TyP/IU/TS7NOpVrWT9aSLL/Z"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"FTuO-iiPyFkF","colab_type":"code","outputId":"c373bfbb-c9bd-4cd0-9550-974748947049","executionInfo":{"status":"ok","timestamp":1583901310960,"user_tz":-330,"elapsed":7394,"user":{"displayName":"Chinchili NagaGayathri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBwe3rdcMMP7AYp4wt-watfkQXNaXTZsR5jnmS=s64","userId":"00972754971743836265"}},"colab":{"base_uri":"https://localhost:8080/","height":165}},"source":["\n","import os\n","from sklearn.preprocessing import scale\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics import confusion_matrix\n","\n","import en_core_web_sm\n","nlp = en_core_web_sm.load()\n","import nltk\n","from nltk.stem import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","import re\n","import pandas as pd\n","import numpy as np\n","from time import time \n","import multiprocessing\n","from gensim.models import Word2Vec\n","from sklearn.manifold import TSNE\n","from sklearn.model_selection import train_test_split\n","\n","\n","import keras \n","from keras.models import Sequential, Model \n","from keras import layers\n","from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, Input, Embedding\n","from keras.layers.merge import Concatenate\n","\n","\n","nltk.download('wordnet')\n","\n","\n","nltk.download('punkt')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"prepoe1tyLOc","colab_type":"code","colab":{}},"source":["def getTokensFromFiles(filePath, label):\n","  main=[]\n","  for i in os.listdir(filePath):\n","    if i != \".ipynb_checkpoints\":\n","      x=i\n","      try:\n","        fname = open(filePath +\"/\"+ x).read()\n","      except UnicodeDecodeError:\n","        continue\n","          \n","      f1=re.sub(r'\\W+', ' ',fname)\n","      \n","      wordnet_lemmatizer = WordNetLemmatizer()\n","      \n","      words = nltk.word_tokenize(f1)\n","      \n","      f2=\" \"\n","      for w in words:\n","        f2=f2+\" \"+wordnet_lemmatizer.lemmatize(w)\n","    \n","      doc = nlp(f2)\n","      \n","      tokens = [token.text for token in doc if not token.is_stop]\n","      \n","      l=[]\n","      l.append(tokens)\n","      if label != -1:\n","        l.append(label)\n","      main.append(l)\n","\n","  return main\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VhW9DaAlyPhY","colab_type":"code","colab":{}},"source":["\n","def trainw2v(trainSet):\n","  cores = multiprocessing.cpu_count() \n","  w2v_model = Word2Vec(min_count=20,\n","                       window=2,\n","                       size=1000,\n","                       sample=6e-5, \n","                       alpha=0.03, \n","                       min_alpha=0.0007, \n","                       negative=20,\n","                       workers=cores-1)\n","\n","  t = time()\n","  w2v_model.build_vocab(trainSet[\"Tokens\"], progress_per=1000)\n","  print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n","\n","  w2v_model.train(trainSet[\"Tokens\"], total_examples=w2v_model.corpus_count, epochs=10000, report_delay=1)\n","  print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n","\n","  return w2v_model;"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E0sNuo1EyPjv","colab_type":"code","colab":{}},"source":["def buildWordVector(w2v_model, tfidf, tokens, size):\n","    vec = np.zeros(size).reshape((1, size))\n","    \n","    count = 0.\n","    for word in tokens:\n","        try:\n","    \n","            vec += w2v_model[word].reshape((1, size)) * tfidf[word]\n","            count += 1.\n","        except KeyError: # handling the case where the token is not\n","                         # in the corpus. useful for testing.\n","            continue\n","    if count != 0:\n","        vec /= count\n","    \n","    \n","    return vec\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ipdFa3TCyPmw","colab_type":"code","colab":{}},"source":["\n","def saveModel(train_vecs_w2v, y_train, test_vecs_w2v, y_test):\n","  dnn_model1 = Sequential()\n","  dnn_model1.add(Dense(128, activation='relu', input_dim=1000))\n","  dnn_model1.add(Dropout(0.7))\n","  dnn_model1.add(Dense(1, activation='sigmoid'))\n","  dnn_model1.compile(optimizer='adadelta',\n","                loss='binary_crossentropy',\n","                metrics=['accuracy'])\n","  dnn_model1.summary()\n","  earlystop = EarlyStopping(monitor = 'loss',\n","                          min_delta = 0,\n","                          patience = 3,\n","                          verbose = 1,\n","                          restore_best_weights = True)  \n","  history = dnn_model1.fit(train_vecs_w2v, y_train, epochs=40, batch_size=50,validation_data=(test_vecs_w2v,y_test))\n","  loss, accuracy = dnn_model1.evaluate(train_vecs_w2v, y_train, verbose=False)\n","  print(\"Training Accuracy: {:.4f}\".format(accuracy))\n","  loss, accuracy = dnn_model1.evaluate(test_vecs_w2v, y_test, verbose=False)\n","  print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n","  dnn_model1.save(\"/content/sample_data/DNN_Model2\", overwrite=True, include_optimizer=True)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lL6cG1iRyPop","colab_type":"code","colab":{}},"source":["\n","#Preparing tokens from raw data\n","\n","training_test = []\n","training_test = training_test + getTokensFromFiles(\"/content/drive/My Drive/resume/Training & Test/Resume\", 1)\n","training_test = training_test + getTokensFromFiles(\"/content/drive/My Drive/resume/Training & Test/Non Resume\", 0)\n","prediction = getTokensFromFiles(\"/content/drive/My Drive/resume/Prediction/Resume\", -1)\n","prediction = prediction + getTokensFromFiles(\"/content/drive/My Drive/resume/Prediction/Non Resume\", -1)\n","\n","\n","#Creating Dataframes for Training & prediction tokens\n","\n","\n","train_df = pd.DataFrame(training_test, columns = ['Tokens', 'Label'])\n","pred_df = pd.DataFrame(prediction, columns = ['Tokens'])\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w-MHaEs9yPlw","colab_type":"code","outputId":"514a4843-a6c4-4f08-99d0-787670c45331","executionInfo":{"status":"error","timestamp":1583901951805,"user_tz":-330,"elapsed":33173,"user":{"displayName":"Chinchili NagaGayathri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBwe3rdcMMP7AYp4wt-watfkQXNaXTZsR5jnmS=s64","userId":"00972754971743836265"}},"colab":{"base_uri":"https://localhost:8080/","height":428}},"source":["#Train Word2Vec model with personal data\n","\n","from gensim.models import KeyedVectors\n","from gensim.models import Word2Vec\n","\n","\n","trained_w2v_model =  KeyedVectors.load_word2vec_format('/content/sample_data/GoogleNews-vectors-negative300.bin.gz', binary=True)\n","\n","y = train_df['Label'].values\n","X = np.array(train_df[\"Tokens\"])\n","\n","#Split the training and test set in provided ratio\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n","X_pred = np.array(pred_df[\"Tokens\"])\n","\n","vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n","\n","matrix = vectorizer.fit_transform([x for x in X_train])\n","\n","tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n","print ('vocab size :', len(tfidf))\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"},{"output_type":"error","ename":"EOFError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-81cc7046697e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrained_w2v_model\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/sample_data/GoogleNews-vectors-negative300.bin.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1436\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    210\u001b[0m                         \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                 \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m                 \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEBADF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read() on write-only GzipFile object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    480\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m                 raise EOFError(\"Compressed file ended before the \"\n\u001b[0m\u001b[1;32m    483\u001b[0m                                \"end-of-stream marker was reached\")\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mEOFError\u001b[0m: Compressed file ended before the end-of-stream marker was reached"]}]},{"cell_type":"code","metadata":{"id":"iYPlTVCRyY1Y","colab_type":"code","outputId":"40a08aa6-7aee-48b0-cfb5-bb71e3433315","executionInfo":{"status":"ok","timestamp":1583835951275,"user_tz":-330,"elapsed":1576,"user":{"displayName":"Chinchili NagaGayathri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBwe3rdcMMP7AYp4wt-watfkQXNaXTZsR5jnmS=s64","userId":"00972754971743836265"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["#Build the word vectors for train, test & prediction dataset \n","train_vecs_w2v = np.concatenate([buildWordVector(trained_w2v_model, tfidf , z, 1000) for z in map(lambda x: x, X_train)])\n","test_vecs_w2v = np.concatenate([buildWordVector(trained_w2v_model, tfidf , z, 1000) for z in map(lambda x: x, X_test)])\n","pred_vec_w2v = np.concatenate([buildWordVector(trained_w2v_model, tfidf , z, 1000) for z in map(lambda x: x, X_pred)])\n","\n","#Scale the vectors\n","train_vecs_w2v = scale(train_vecs_w2v)\n","test_vecs_w2v = scale(test_vecs_w2v)\n","pred_vec_w2v = scale(pred_vec_w2v)\n","\n","\n","print ('shape for training set : ',train_vecs_w2v.shape,\n","      '\\nshape for test set : ', test_vecs_w2v.shape,\n","       '\\nshape for predicted set : ', pred_vec_w2v.shape,) \n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  \n"],"name":"stderr"},{"output_type":"stream","text":["shape for training set :  (135, 1000) \n","shape for test set :  (58, 1000) \n","shape for predicted set :  (7, 1000)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qEOF8MMjydcw","colab_type":"code","outputId":"f49a38f9-c961-4980-bcd6-ccd59743c47f","executionInfo":{"status":"ok","timestamp":1583835960086,"user_tz":-330,"elapsed":2748,"user":{"displayName":"Chinchili NagaGayathri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBwe3rdcMMP7AYp4wt-watfkQXNaXTZsR5jnmS=s64","userId":"00972754971743836265"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from keras.callbacks import EarlyStopping \n","saveModel(train_vecs_w2v, y_train, test_vecs_w2v, y_test)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n","Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_3 (Dense)              (None, 128)               128128    \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 1)                 129       \n","=================================================================\n","Total params: 128,257\n","Trainable params: 128,257\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 135 samples, validate on 58 samples\n","Epoch 1/40\n","135/135 [==============================] - 0s 3ms/step - loss: 0.6693 - acc: 0.7185 - val_loss: 0.1548 - val_acc: 0.9828\n","Epoch 2/40\n","135/135 [==============================] - 0s 164us/step - loss: 0.1966 - acc: 0.9333 - val_loss: 0.0977 - val_acc: 0.9655\n","Epoch 3/40\n","135/135 [==============================] - 0s 188us/step - loss: 0.1613 - acc: 0.9407 - val_loss: 0.0812 - val_acc: 0.9828\n","Epoch 4/40\n","135/135 [==============================] - 0s 201us/step - loss: 0.0747 - acc: 0.9852 - val_loss: 0.0725 - val_acc: 0.9828\n","Epoch 5/40\n","135/135 [==============================] - 0s 176us/step - loss: 0.0674 - acc: 0.9704 - val_loss: 0.0672 - val_acc: 0.9828\n","Epoch 6/40\n","135/135 [==============================] - 0s 174us/step - loss: 0.0688 - acc: 0.9778 - val_loss: 0.0621 - val_acc: 0.9828\n","Epoch 7/40\n","135/135 [==============================] - 0s 166us/step - loss: 0.0428 - acc: 0.9926 - val_loss: 0.0620 - val_acc: 0.9828\n","Epoch 8/40\n","135/135 [==============================] - 0s 158us/step - loss: 0.0407 - acc: 0.9778 - val_loss: 0.0599 - val_acc: 0.9828\n","Epoch 9/40\n","135/135 [==============================] - 0s 184us/step - loss: 0.0163 - acc: 1.0000 - val_loss: 0.0557 - val_acc: 0.9828\n","Epoch 10/40\n","135/135 [==============================] - 0s 156us/step - loss: 0.0292 - acc: 1.0000 - val_loss: 0.0578 - val_acc: 0.9828\n","Epoch 11/40\n","135/135 [==============================] - 0s 183us/step - loss: 0.0129 - acc: 1.0000 - val_loss: 0.0588 - val_acc: 0.9828\n","Epoch 12/40\n","135/135 [==============================] - 0s 153us/step - loss: 0.0094 - acc: 1.0000 - val_loss: 0.0594 - val_acc: 0.9828\n","Epoch 13/40\n","135/135 [==============================] - 0s 187us/step - loss: 0.0156 - acc: 1.0000 - val_loss: 0.0655 - val_acc: 0.9828\n","Epoch 14/40\n","135/135 [==============================] - 0s 194us/step - loss: 0.0208 - acc: 0.9926 - val_loss: 0.0681 - val_acc: 0.9828\n","Epoch 15/40\n","135/135 [==============================] - 0s 202us/step - loss: 0.0158 - acc: 0.9926 - val_loss: 0.0610 - val_acc: 0.9828\n","Epoch 16/40\n","135/135 [==============================] - 0s 172us/step - loss: 0.0100 - acc: 1.0000 - val_loss: 0.0614 - val_acc: 0.9828\n","Epoch 17/40\n","135/135 [==============================] - 0s 176us/step - loss: 0.0094 - acc: 1.0000 - val_loss: 0.0627 - val_acc: 0.9828\n","Epoch 18/40\n","135/135 [==============================] - 0s 156us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 0.0651 - val_acc: 0.9828\n","Epoch 19/40\n","135/135 [==============================] - 0s 224us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 0.0656 - val_acc: 0.9828\n","Epoch 20/40\n","135/135 [==============================] - 0s 180us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.0651 - val_acc: 0.9828\n","Epoch 21/40\n","135/135 [==============================] - 0s 181us/step - loss: 0.0147 - acc: 1.0000 - val_loss: 0.0629 - val_acc: 0.9828\n","Epoch 22/40\n","135/135 [==============================] - 0s 164us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.0640 - val_acc: 0.9828\n","Epoch 23/40\n","135/135 [==============================] - 0s 170us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 0.0664 - val_acc: 0.9828\n","Epoch 24/40\n","135/135 [==============================] - 0s 150us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.0683 - val_acc: 0.9828\n","Epoch 25/40\n","135/135 [==============================] - 0s 172us/step - loss: 0.0111 - acc: 1.0000 - val_loss: 0.0715 - val_acc: 0.9828\n","Epoch 26/40\n","135/135 [==============================] - 0s 160us/step - loss: 0.0076 - acc: 1.0000 - val_loss: 0.0703 - val_acc: 0.9828\n","Epoch 27/40\n","135/135 [==============================] - 0s 164us/step - loss: 0.0170 - acc: 0.9852 - val_loss: 0.0726 - val_acc: 0.9828\n","Epoch 28/40\n","135/135 [==============================] - 0s 178us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.0744 - val_acc: 0.9828\n","Epoch 29/40\n","135/135 [==============================] - 0s 198us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 0.0780 - val_acc: 0.9828\n","Epoch 30/40\n","135/135 [==============================] - 0s 207us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.0787 - val_acc: 0.9828\n","Epoch 31/40\n","135/135 [==============================] - 0s 216us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.0780 - val_acc: 0.9828\n","Epoch 32/40\n","135/135 [==============================] - 0s 202us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0782 - val_acc: 0.9828\n","Epoch 33/40\n","135/135 [==============================] - 0s 202us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0790 - val_acc: 0.9828\n","Epoch 34/40\n","135/135 [==============================] - 0s 198us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 0.0809 - val_acc: 0.9828\n","Epoch 35/40\n","135/135 [==============================] - 0s 191us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.0823 - val_acc: 0.9828\n","Epoch 36/40\n","135/135 [==============================] - 0s 239us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.0808 - val_acc: 0.9828\n","Epoch 37/40\n","135/135 [==============================] - 0s 226us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.0812 - val_acc: 0.9828\n","Epoch 38/40\n","135/135 [==============================] - 0s 216us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 0.0851 - val_acc: 0.9828\n","Epoch 39/40\n","135/135 [==============================] - 0s 185us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0853 - val_acc: 0.9828\n","Epoch 40/40\n","135/135 [==============================] - 0s 186us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.0826 - val_acc: 0.9828\n","Training Accuracy: 1.0000\n","Testing Accuracy:  0.9828\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1O8OO39MyfmI","colab_type":"code","outputId":"94f0d584-a5a1-4b76-b851-a26de1dae85f","executionInfo":{"status":"ok","timestamp":1583835966939,"user_tz":-330,"elapsed":2297,"user":{"displayName":"Chinchili NagaGayathri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBwe3rdcMMP7AYp4wt-watfkQXNaXTZsR5jnmS=s64","userId":"00972754971743836265"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["from keras.models import load_model\n","\n","trained_model = load_model(\"/content/sample_data/DNN_Model2\")\n","\n","l = trained_model.predict(pred_vec_w2v, verbose=True)\n","print(l)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n","7/7 [==============================] - 0s 13ms/step\n","[[9.9996626e-01]\n"," [9.9995935e-01]\n"," [4.3215469e-02]\n"," [9.9999976e-01]\n"," [5.1797028e-03]\n"," [7.1375431e-03]\n"," [9.3743120e-06]]\n"],"name":"stdout"}]}]}