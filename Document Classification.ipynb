{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Document Classification.ipynb","provenance":[{"file_id":"130igEs5W2JoWILOyDCITTV6DIYHtqY_E","timestamp":1583203906436},{"file_id":"1Oei4kVWfB_Oqwh9oxMdLxsRYOt1rp7cD","timestamp":1583121408998}],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1J0YPIZ7ADPof6M2AIgDD1hmTwKEXSjtU","authorship_tag":"ABX9TyMDrDgY6AJGBlduaSh2W1hj"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"HqTnM5CWTA2Y","outputId":"f1ce5212-7f18-4360-a112-395482158171","executionInfo":{"status":"error","timestamp":1583489195503,"user_tz":-330,"elapsed":7509,"user":{"displayName":"Chinchili NagaGayathri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBwe3rdcMMP7AYp4wt-watfkQXNaXTZsR5jnmS=s64","userId":"00972754971743836265"}},"colab":{"base_uri":"https://localhost:8080/","height":663}},"source":["import en_core_web_sm\n","nlp = en_core_web_sm.load()\n","import nltk\n","from nltk.stem import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","import re\n","import pandas as pd\n","import numpy as np\n","from time import time \n","import multiprocessing\n","from gensim.models import Word2Vec\n","from sklearn.manifold import TSNE\n","from sklearn.model_selection import train_test_split\n","\n","#file_name = 'document_classification\\Resume1.txt'\n","#mat=[][2]\n","import os\n","\n","\n","main=[]\n","#print(files)\n","for i in os.listdir(\"/content/drive/My Drive/resume/Training & Test/Resume\"):\n","     if i != \".ipynb_checkpoints\":\n","      x=i\n","      try:\n","        fname = open(\"/content/drive/My Drive/resume/Training & Test/Resume/\"+x).read()\n","      except UnicodeDecodeError:\n","         continue\n","        \n","      f1=re.sub(r'\\W+', ' ',fname)\n","    \n","      wordnet_lemmatizer = WordNetLemmatizer()\n","    \n","      words = nltk.word_tokenize(f1)\n","    \n","      f2=\" \"\n","      for w in words:\n","          f2=f2+\" \"+wordnet_lemmatizer.lemmatize(w)\n","    \n","      doc = nlp(f2)\n","    \n","      tokens = [token.text for token in doc if not token.is_stop]\n","    \n","      l=[]\n","      l.append(tokens)\n","      l.append(\"1\")\n","      main.append(l)\n","\n","for i in os.listdir(\"/content/drive/My Drive/resume/Training & Test/Non Resume\"):\n","     if i != \".ipynb_checkpoints\":\n","       x=i\n","       try:\n","          fname = open(\"/content/drive/My Drive/resume/Training & Test/Non Resume/\"+x).read()\n","       except UnicodeDecodeError:\n","          continue\n","        \n","       f1=re.sub(r'\\W+', ' ',fname)\n","       wordnet_lemmatizer = WordNetLemmatizer()\n","       words = nltk.word_tokenize(f1)\n","       f2=\" \"\n","       for w in words:\n","           f2=f2+\" \"+wordnet_lemmatizer.lemmatize(w)\n","       doc = nlp(f2)\n","    \n","       tokens = [token.text for token in doc if not token.is_stop]\n","      \n","       l=[]\n","       l.append(tokens)\n","       l.append(\"0\")\n","       main.append(l)\n","\n","\n","\n","df = pd.DataFrame(main, columns = ['Tokens', 'Label'])\n","\n","\n","\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"error","ename":"LookupError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-4d13f0f81f42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0mwordnet_lemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m       \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n    - ''\n**********************************************************************\n"]}]},{"cell_type":"code","metadata":{"id":"l28_Q3_IjmGe","colab_type":"code","outputId":"57ec8d3a-51b6-4664-c5e4-af5f8b11f057","executionInfo":{"status":"ok","timestamp":1583471820584,"user_tz":-330,"elapsed":2983,"user":{"displayName":"Chinchili NagaGayathri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBwe3rdcMMP7AYp4wt-watfkQXNaXTZsR5jnmS=s64","userId":"00972754971743836265"}},"colab":{"base_uri":"https://localhost:8080/","height":235}},"source":["\n","pred = []\n","for i in os.listdir(\"/content/drive/My Drive/resume/Prediction/Resume\"):\n","    if i != \".ipynb_checkpoints\":\n","      x=i\n","      try:\n","        fname = open(\"/content/drive/My Drive/resume/Prediction/Resume/\"+x).read()\n","      except UnicodeDecodeError:\n","        continue\n","        \n","      f1=re.sub(r'\\W+', ' ',fname)\n","      wordnet_lemmatizer = WordNetLemmatizer()\n","      words = nltk.word_tokenize(f1)\n","      f2=\" \"\n","      for w in words:\n","          f2=f2+\" \"+wordnet_lemmatizer.lemmatize(w)\n","      doc = nlp(f2)\n","   \n","      tokens = [token.text for token in doc if not token.is_stop]\n","    \n","      l=[]\n","      l.append(tokens)\n","      pred.append(l)\n","\n","for i in os.listdir(\"/content/drive/My Drive/resume/Prediction/Non Resume\"):\n","    if i != \".ipynb_checkpoints\":\n","      x=i\n","      try:\n","          fname = open(\"/content/drive/My Drive/resume/Prediction/Non Resume/\"+x).read()\n","      except UnicodeDecodeError:\n","          continue\n","          \n","      f1=re.sub(r'\\W+', ' ',fname)\n","      wordnet_lemmatizer = WordNetLemmatizer()\n","      words = nltk.word_tokenize(f1)\n","      f2=\" \"\n","      for w in words:\n","          f2=f2+\" \"+wordnet_lemmatizer.lemmatize(w)\n","      doc = nlp(f2)\n","    \n","      tokens = [token.text for token in doc if not token.is_stop]\n","      \n","      l=[]\n","      l.append(tokens)\n","      pred.append(l)\n","\n","\n","df_pred = pd.DataFrame(pred, columns=['Tokens'])\n","df_pred"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tokens</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[  , Computer, Science, Faculty, New, Delhi, D...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[  , Isabelle, Smith, rn, cphon, Sometown, TX,...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[  , BIODATA, G, Pranathi, 8143958515, 16wh1a0...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[  , PAYMENT, RECEIPT, Guest, Sanjay, Date, 31...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[  , Cody, Fredrickson, 123, 456, 7891, cfredr...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>[  , Jax, Sampson, 111, 789, 3456, jax, sampso...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              Tokens\n","0  [  , Computer, Science, Faculty, New, Delhi, D...\n","1  [  , Isabelle, Smith, rn, cphon, Sometown, TX,...\n","2  [  , BIODATA, G, Pranathi, 8143958515, 16wh1a0...\n","3  [  , PAYMENT, RECEIPT, Guest, Sanjay, Date, 31...\n","4  [  , Cody, Fredrickson, 123, 456, 7891, cfredr...\n","5  [  , Jax, Sampson, 111, 789, 3456, jax, sampso..."]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"em5cKGYBkGOV","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"TH5DsNhckGJT","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"6XPpIa9cttD2","colab_type":"code","outputId":"fe78fe1f-0e2c-4a86-d34f-62d7462dcaf2","executionInfo":{"status":"ok","timestamp":1583469449157,"user_tz":-330,"elapsed":1774,"user":{"displayName":"Chinchili NagaGayathri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBwe3rdcMMP7AYp4wt-watfkQXNaXTZsR5jnmS=s64","userId":"00972754971743836265"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["import nltk\n","\n","nltk.download('wordnet')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"8Kn3wH3AIl-_","colab_type":"code","outputId":"0de0f6a4-84e2-42d1-a09d-1b18003a4467","executionInfo":{"status":"ok","timestamp":1583832602183,"user_tz":-330,"elapsed":3642,"user":{"displayName":"Chinchili NagaGayathri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBwe3rdcMMP7AYp4wt-watfkQXNaXTZsR5jnmS=s64","userId":"00972754971743836265"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["import nltk\n","\n","nltk.download('punkt')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"pZI5eCz7_yUL","colab_type":"code","outputId":"0bc03980-832f-4b53-fc63-dde434fdc0ec","executionInfo":{"status":"error","timestamp":1583488901098,"user_tz":-330,"elapsed":1266,"user":{"displayName":"Chinchili NagaGayathri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBwe3rdcMMP7AYp4wt-watfkQXNaXTZsR5jnmS=s64","userId":"00972754971743836265"}},"colab":{"base_uri":"https://localhost:8080/","height":231}},"source":["cores = multiprocessing.cpu_count() \n","w2v_model = Word2Vec(min_count=20,\n","                     window=2,\n","                     size=300,\n","                     sample=6e-5, \n","                     alpha=0.03, \n","                     min_alpha=0.0007, \n","                     negative=20,\n","                     workers=cores-1)\n","\n","t = time()\n","\n","w2v_model.build_vocab(df[\"Tokens\"], progress_per=1000)\n","print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n","\n","print(w2v_model.corpus_count)\n","w2v_model.train(df[\"Tokens\"], total_examples=w2v_model.corpus_count, epochs=10000, report_delay=1)\n","print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-c498f00ecc1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m w2v_model = Word2Vec(min_count=20,\n\u001b[1;32m      3\u001b[0m                      \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                      \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                      \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6e-5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'multiprocessing' is not defined"]}]},{"cell_type":"code","metadata":{"id":"6JsmKsCu6QGP","colab_type":"code","outputId":"0ac78bea-5960-460f-a3fd-f0b644d26c21","executionInfo":{"status":"ok","timestamp":1583472524304,"user_tz":-330,"elapsed":1446,"user":{"displayName":"Chinchili NagaGayathri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBwe3rdcMMP7AYp4wt-watfkQXNaXTZsR5jnmS=s64","userId":"00972754971743836265"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from sklearn.manifold import TSNE\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import scale\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics import confusion_matrix\n","\n","y = df['Label'].values\n","X = np.array(df[\"Tokens\"])\n","\n","#And here is the train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n","\n","vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n","\n","matrix = vectorizer.fit_transform([x for x in X_train])\n","\n","tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n","print ('vocab size :', len(tfidf))\n","\n","def buildWordVector(tokens, size):\n","    vec = np.zeros(size).reshape((1, size))\n","    \n","    count = 0.\n","    for word in tokens:\n","        try:\n","    \n","            vec += w2v_model[word].reshape((1, size)) * tfidf[word]\n","            count += 1.\n","        except KeyError: # handling the case where the token is not\n","                         # in the corpus. useful for testing.\n","            continue\n","    if count != 0:\n","        vec /= count\n","    \n","    \n","    return vec\n","\n","train_vecs_w2v = np.concatenate([buildWordVector(z, 300) for z in map(lambda x: x, X_train)])\n","\n","\n","test_vecs_w2v = np.concatenate([buildWordVector(z, 300) for z in map(lambda x: x, X_test)])\n","train_vecs_w2v = scale(train_vecs_w2v)\n","\n","\n","test_vecs_w2v = scale(test_vecs_w2v)\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["vocab size : 641\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"_qEgCPSc6QKb","colab_type":"code","outputId":"7abc9843-9b48-47a1-bb64-f016058865a0","executionInfo":{"status":"ok","timestamp":1583472530241,"user_tz":-330,"elapsed":1117,"user":{"displayName":"Chinchili NagaGayathri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBwe3rdcMMP7AYp4wt-watfkQXNaXTZsR5jnmS=s64","userId":"00972754971743836265"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["\n","\n","X_pred = np.array(df_pred[\"Tokens\"])\n","\n","\n","pred_vec_w2v = np.concatenate([buildWordVector(z, 300) for z in map(lambda x: x, X_pred)])\n","pred_vec_w2v = scale(pred_vec_w2v)\n","\n","\n","print ('shape for training set : ',train_vecs_w2v.shape,\n","      '\\nshape for test set : ', test_vecs_w2v.shape,\n","       '\\nshape for predicted set : ', pred_vec_w2v.shape,) \n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["shape for training set :  (135, 300) \n","shape for test set :  (59, 300) \n","shape for predicted set :  (6, 300)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"JOXnxWbK6QOz","colab_type":"code","outputId":"a04ae17e-bf0d-46e8-80e0-8d7df8cc8763","executionInfo":{"status":"ok","timestamp":1583472608315,"user_tz":-330,"elapsed":1383,"user":{"displayName":"Chinchili NagaGayathri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBwe3rdcMMP7AYp4wt-watfkQXNaXTZsR5jnmS=s64","userId":"00972754971743836265"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["import keras \n","from keras.models import Sequential, Model \n","from keras import layers\n","from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, Input, Embedding\n","from keras.layers.merge import Concatenate\n","\n","model1 = Sequential()\n"," \n","model1.add(Dense(128, activation='relu', input_dim=300))\n","model1.add(Dropout(0.7))\n","model1.add(Dense(1, activation='sigmoid'))\n","model1.compile(optimizer='adadelta',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","model1.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n","Model: \"sequential_4\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_7 (Dense)              (None, 128)               38528     \n","_________________________________________________________________\n","dropout_4 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 1)                 129       \n","=================================================================\n","Total params: 38,657\n","Trainable params: 38,657\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OZ1o9woa73t2","colab_type":"code","outputId":"6de12e3d-4dcd-41f6-933f-753eeb7f3011","executionInfo":{"status":"ok","timestamp":1583472613573,"user_tz":-330,"elapsed":2240,"user":{"displayName":"Chinchili NagaGayathri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBwe3rdcMMP7AYp4wt-watfkQXNaXTZsR5jnmS=s64","userId":"00972754971743836265"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import matplotlib.pyplot as plt\n","\n","history = model1.fit(train_vecs_w2v, y_train, epochs=40, batch_size=50,\n","                   validation_data=(test_vecs_w2v,y_test))\n","loss, accuracy = model1.evaluate(train_vecs_w2v, y_train, verbose=False)\n","print(\"Training Accuracy: {:.4f}\".format(accuracy))\n","\n","\n","loss, accuracy = model1.evaluate(test_vecs_w2v, y_test, verbose=False)\n","print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 135 samples, validate on 59 samples\n","Epoch 1/40\n","135/135 [==============================] - 0s 3ms/step - loss: 1.1296 - acc: 0.5556 - val_loss: 0.4292 - val_acc: 0.7288\n","Epoch 2/40\n","135/135 [==============================] - 0s 88us/step - loss: 0.4618 - acc: 0.7778 - val_loss: 0.2755 - val_acc: 0.8644\n","Epoch 3/40\n","135/135 [==============================] - 0s 80us/step - loss: 0.3272 - acc: 0.8444 - val_loss: 0.2178 - val_acc: 0.8983\n","Epoch 4/40\n","135/135 [==============================] - 0s 101us/step - loss: 0.2571 - acc: 0.8667 - val_loss: 0.1775 - val_acc: 0.9153\n","Epoch 5/40\n","135/135 [==============================] - 0s 89us/step - loss: 0.1878 - acc: 0.9259 - val_loss: 0.1538 - val_acc: 0.9322\n","Epoch 6/40\n","135/135 [==============================] - 0s 90us/step - loss: 0.2023 - acc: 0.9037 - val_loss: 0.1407 - val_acc: 0.9492\n","Epoch 7/40\n","135/135 [==============================] - 0s 96us/step - loss: 0.1837 - acc: 0.9407 - val_loss: 0.1318 - val_acc: 0.9492\n","Epoch 8/40\n","135/135 [==============================] - 0s 90us/step - loss: 0.0971 - acc: 0.9704 - val_loss: 0.1237 - val_acc: 0.9492\n","Epoch 9/40\n","135/135 [==============================] - 0s 98us/step - loss: 0.1230 - acc: 0.9630 - val_loss: 0.1118 - val_acc: 0.9661\n","Epoch 10/40\n","135/135 [==============================] - 0s 95us/step - loss: 0.0742 - acc: 0.9778 - val_loss: 0.1083 - val_acc: 0.9661\n","Epoch 11/40\n","135/135 [==============================] - 0s 137us/step - loss: 0.1024 - acc: 0.9778 - val_loss: 0.1037 - val_acc: 0.9661\n","Epoch 12/40\n","135/135 [==============================] - 0s 88us/step - loss: 0.0631 - acc: 0.9852 - val_loss: 0.0993 - val_acc: 0.9661\n","Epoch 13/40\n","135/135 [==============================] - 0s 84us/step - loss: 0.0744 - acc: 0.9778 - val_loss: 0.0960 - val_acc: 0.9661\n","Epoch 14/40\n","135/135 [==============================] - 0s 88us/step - loss: 0.0488 - acc: 0.9852 - val_loss: 0.0941 - val_acc: 0.9661\n","Epoch 15/40\n","135/135 [==============================] - 0s 97us/step - loss: 0.0567 - acc: 0.9852 - val_loss: 0.0931 - val_acc: 0.9661\n","Epoch 16/40\n","135/135 [==============================] - 0s 98us/step - loss: 0.0655 - acc: 0.9852 - val_loss: 0.0909 - val_acc: 0.9661\n","Epoch 17/40\n","135/135 [==============================] - 0s 113us/step - loss: 0.0519 - acc: 0.9926 - val_loss: 0.0894 - val_acc: 0.9661\n","Epoch 18/40\n","135/135 [==============================] - 0s 105us/step - loss: 0.0399 - acc: 0.9852 - val_loss: 0.0884 - val_acc: 0.9661\n","Epoch 19/40\n","135/135 [==============================] - 0s 100us/step - loss: 0.0494 - acc: 0.9926 - val_loss: 0.0860 - val_acc: 0.9661\n","Epoch 20/40\n","135/135 [==============================] - 0s 103us/step - loss: 0.0301 - acc: 1.0000 - val_loss: 0.0859 - val_acc: 0.9661\n","Epoch 21/40\n","135/135 [==============================] - 0s 100us/step - loss: 0.0447 - acc: 0.9926 - val_loss: 0.0825 - val_acc: 0.9661\n","Epoch 22/40\n","135/135 [==============================] - 0s 100us/step - loss: 0.0293 - acc: 1.0000 - val_loss: 0.0809 - val_acc: 0.9661\n","Epoch 23/40\n","135/135 [==============================] - 0s 107us/step - loss: 0.0273 - acc: 1.0000 - val_loss: 0.0796 - val_acc: 0.9661\n","Epoch 24/40\n","135/135 [==============================] - 0s 101us/step - loss: 0.0216 - acc: 1.0000 - val_loss: 0.0789 - val_acc: 0.9661\n","Epoch 25/40\n","135/135 [==============================] - 0s 97us/step - loss: 0.0290 - acc: 1.0000 - val_loss: 0.0794 - val_acc: 0.9661\n","Epoch 26/40\n","135/135 [==============================] - 0s 96us/step - loss: 0.0294 - acc: 1.0000 - val_loss: 0.0786 - val_acc: 0.9661\n","Epoch 27/40\n","135/135 [==============================] - 0s 96us/step - loss: 0.0232 - acc: 1.0000 - val_loss: 0.0785 - val_acc: 0.9661\n","Epoch 28/40\n","135/135 [==============================] - 0s 85us/step - loss: 0.0245 - acc: 1.0000 - val_loss: 0.0766 - val_acc: 0.9661\n","Epoch 29/40\n","135/135 [==============================] - 0s 81us/step - loss: 0.0204 - acc: 1.0000 - val_loss: 0.0755 - val_acc: 0.9661\n","Epoch 30/40\n","135/135 [==============================] - 0s 80us/step - loss: 0.0190 - acc: 1.0000 - val_loss: 0.0747 - val_acc: 0.9831\n","Epoch 31/40\n","135/135 [==============================] - 0s 76us/step - loss: 0.0238 - acc: 1.0000 - val_loss: 0.0742 - val_acc: 0.9831\n","Epoch 32/40\n","135/135 [==============================] - 0s 85us/step - loss: 0.0185 - acc: 0.9926 - val_loss: 0.0752 - val_acc: 0.9831\n","Epoch 33/40\n","135/135 [==============================] - 0s 91us/step - loss: 0.0198 - acc: 1.0000 - val_loss: 0.0754 - val_acc: 0.9831\n","Epoch 34/40\n","135/135 [==============================] - 0s 122us/step - loss: 0.0278 - acc: 0.9926 - val_loss: 0.0746 - val_acc: 0.9831\n","Epoch 35/40\n","135/135 [==============================] - 0s 90us/step - loss: 0.0108 - acc: 1.0000 - val_loss: 0.0754 - val_acc: 0.9831\n","Epoch 36/40\n","135/135 [==============================] - 0s 97us/step - loss: 0.0126 - acc: 1.0000 - val_loss: 0.0753 - val_acc: 0.9831\n","Epoch 37/40\n","135/135 [==============================] - 0s 100us/step - loss: 0.0133 - acc: 1.0000 - val_loss: 0.0748 - val_acc: 0.9831\n","Epoch 38/40\n","135/135 [==============================] - 0s 97us/step - loss: 0.0112 - acc: 1.0000 - val_loss: 0.0745 - val_acc: 0.9831\n","Epoch 39/40\n","135/135 [==============================] - 0s 101us/step - loss: 0.0100 - acc: 1.0000 - val_loss: 0.0744 - val_acc: 0.9831\n","Epoch 40/40\n","135/135 [==============================] - 0s 90us/step - loss: 0.0181 - acc: 0.9926 - val_loss: 0.0749 - val_acc: 0.9831\n","Training Accuracy: 1.0000\n","Testing Accuracy:  0.9831\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1Nu3tHI573re","colab_type":"code","outputId":"07abb250-1cc7-429b-a53c-9007006f95b9","executionInfo":{"status":"ok","timestamp":1583472616752,"user_tz":-330,"elapsed":996,"user":{"displayName":"Chinchili NagaGayathri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBwe3rdcMMP7AYp4wt-watfkQXNaXTZsR5jnmS=s64","userId":"00972754971743836265"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["\n","l = model1.predict(pred_vec_w2v, verbose=False)\n","print(l)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[9.9992025e-01]\n"," [9.9994910e-01]\n"," [1.0941011e-02]\n"," [2.7500442e-01]\n"," [7.1451475e-04]\n"," [8.6270487e-03]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tQeVCdRUHUMU","colab_type":"code","outputId":"3c693d0e-a4ed-4de6-f76d-cf0ab56af148","executionInfo":{"status":"ok","timestamp":1583492457575,"user_tz":-330,"elapsed":2893,"user":{"displayName":"Chinchili NagaGayathri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBwe3rdcMMP7AYp4wt-watfkQXNaXTZsR5jnmS=s64","userId":"00972754971743836265"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["\n","! ls sample_data/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["anscombe.json\t\t      mnist_test.csv\n","california_housing_test.csv   mnist_train_small.csv\n","california_housing_train.csv  README.md\n"],"name":"stdout"}]}]}